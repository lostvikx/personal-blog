---
title: "In a World of AI Systems"
author: "Vikram S. Negi"
date: 2025-03-18
description: "My thoughts on people willingly reliquishing control to AI systems."
thumbnail: "7e24ccc6a763a46c.png"
---

Algorithms are precise instructions written to solve a problem. But machine learning algorithms are a different beasts. These data-driven algorithms essentially learn the underlying patterns in the training dataset and predict the outputs.

In this post I would like to discuss about the nature of recommendation algorithms. From what I understand is that these models are not just a simple clustering algorithm, but way more complex.

Let's look at an example like the mighty YouTube's recommendation algorithm. Here are some datapoints that it may consider:

1. Watch history
2. Time of day of watching
3. Subscriptions & Watch Later playlist
4. Liked videos
5. Random trending videos in user's location

The ulimate goal here is not preserve or share knowledge about the world, but a way more simple yet terifying goal: retain user's attention. What this leads to is more watch time and thereby increasing the site's advertising revenue. 

This post is partly inspired by a [video](https://youtu.be/QEJpZjg8GuA) about how recommendation algorithms are changing how we think.

## The Internet

The internet was created by universities to share knowledge through disscussions, through email chains and online message boards. It was suppose to be and it certiainly can be the library of human knowledge.

AI is not real and does not possess some inate knowledge of the world. Using AI tools are great to initialize a topic you would like to understand about, but anything other than that could be considered as unethical.

Simple reasoning behind why AI products are unenthical is because it is completely depended on the training dataset. Now if that dataset contains biases they will only be exacerbated, and a bias model is considered bad, as it will not reason about the decision it makes.

Here is an example that showcases the problem. You apply for a job and are from a community that is a minority in your country. You live in a neighborhood that has a high crime rate, now a machine learning model can easily pick-up these patterns and decide to not hire you.

### Relinquishing Control

Decision making is already done by AI models, not this is not a plot of a sci-fi novel. This is aptly explained by Nudge Economics, it suggests that people are too lazy and will use whatever is the default.

These models are controlling what gets shown on your device and what does not. And the things shown are always extreme in nature because that is what drives the most engagement. Users have become simple minded, thinking they share the opinion A or B, and not between them.

Feeds on a website, like social media, were suppose to show content from the people a user followed or subscribed. But things have changed and the "For You" page has become the deafult.

From personal experience, sometimes a video or post that was recommended to me by the model inspires and promotes critical thinking. But unfortunately, this is not the case most of the time.

## Ethical AI

A data scientist may wonder, where should I use these awesome pre-trained models? These models really shine in a playful sense, like video games, sports analysis, stock price predictions, and mainly research purposes. Where it may not inflict harm or promote discrimination directly.

Critical problems like medicine should not actively promote the use of AI models for treatment or diagnosis. I say this because explainability of AI is still a black-box problem. This is also bad from a research perspective, as researchers may arive at conclusions through these models, but don't understand why and how did the model produce those results.

### AI-powered Tools

Tools like Large Language Models (LLMs) are great for writing things that feel like a chore. These can be helpful for students and professionals to be more productive, that is the only use case I believe to be the best. 

We should not ignore the amount of energy in terms of compute power is required to run LLMs model on some server farm. Distilling these models to make them more compute efficient should be a top priority for big-tech.

Over-relying on these tools are not such a great idea as well. A recent [paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/lee_2025_ai_critical_thinking_survey.pdf) published by Microsoft suggests critical thinking skills of professionals have seen a statistically significant negative impact on those that regularly use LLMs.

### AI Research

In recent times, we have noticed an increase in money spent on AI research. But the main researchers are employeed by for-profit companies which is unlike the tradition. Universities and other research-first organizations just do not have the capital to create larges scale AI models.

But, why do we need large AI models and what does it mean to be large? Well, an AI model trained on large amount of data combined with large amounts of compute power required to train them. Just the cost of doing AI research is stopping universities to proceed with it. And I predict this will not change until there is a more resource-efficient way to conduct them, perhaps a change in the underlying algorithms that don't necessarily follow the AI scaling law.

## Conclusion

We are increasingly drifting towards a world where our thoughts and perceptions are subtly shaped by the curated content we consume online. This algorithmic steering can lead us to believe in a singular "correct" way of thinking or acting, eroding our capacity for independent judgment and our control over how we truly feel about the world. It's a quiet surrender of agency, where the digital landscape dictates not just what we see, but what we come to believe.
